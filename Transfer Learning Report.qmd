---
title: "Transfer Learning Report"
author: "Bernie Graves, Kabir Snell, Ao Xu, Yuqing Xia"
format: html
editor: visual
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

## Abstract

Deep learning models can take a lot of time and resources to train, especially when there the amount of data gets large. Transfer learning lets you use pre-trained models on related but different tasks, saving time and resources while still using quality models. We used a pre-trained model from Google called the Universal Sentence Encoder which essentially turns sentences into vectors. First you load the pre trained model and set the already trained layers to not be trainable. Then you make this pre trained model a layer in your deep learning model and train on your new data. Transfer learning has applications beyond sentiment analysis. It can be used with any pre-trained deep learning model that you have access to.

## Dataset

All these Tweets and Comments were extracted using there Respective Apis Tweepy and PRAW. These tweets and Comments Were Made on Narendra Modi and Other Leaders as well as Peoples Opinion Towards the Next Prime Minister of The Nation ( In Context with General Elections Held In India - 2019). All the Tweets and Comments From twitter and Reddit are Cleaned using Pythons re and also NLP with a Sentimental Label to each ranging from -1 to 1.

0 Indicating it is a Neutral Tweet/Comment\
1 Indicating a Positive Sentiment\
-1 Indicating a Negative Tweet/Comment

Twitter.csv Dataset has around 163K Tweets along with Sentiment Labels.\
Reddit.csv Dataset has around 37K Comments along with its Sentimental Label

For the pre-processing, we split the sentimental label into three columns: "positive", "negative", and "neutral". The column has 1 if the original column indicates the sentiment and 0 otherwise, for example, if the original sentiment column has value -1, then for the new sentiment columns, "negative" has value 1, "neutral" and "positive" columns both has 0. Then we split the data sets into test and training sets for model development.

## Pre-Trained Universal Sentence Embedder

## Model Development

```{r}
library(keras)
library(tidyverse)
library(tidymodels)
library(tensorflow)
library(tfhub)

# Loading the pre-trained Universal Sentence Embedder
model.url <- "https://tfhub.dev/google/universal-sentence-encoder/4"
USE.layer <- tfhub::layer_hub(handle = model.url, trainable = FALSE,
                              name = "UniversalSentenceEmbeddingLayer")

# Load in data
load("Data/Data_Processed.RData")

# had to put labels in matrix
train.labels <- as.matrix(twitter.train.labels)
test.labels <- as.matrix(twitter.test.labels)

# Building the Model
model <- keras_model_sequential() %>%
  USE.layer() %>%
  layer_reshape(target_shape = c(-1, 512)) %>%
  bidirectional(layer_lstm(units = 64, return_sequences = T, go_backwards = T)) %>%
  layer_dropout(0.5) %>%
  layer_global_max_pooling_1d() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(0.25) %>%
  layer_dense(1, activation = "sigmoid")


# configure for training
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = 'accuracy'
)

# train
history <- model %>%
  fit(twitter.train.text,
      train.labels,
      validation_split = 0.3,
      epochs = 5)

summary(model)

evaluate(model, twitter.test.text, test.labels)
 

```

For the transfer learning, we first loaded the pre-trained model weights into our base model. Then we freeze the layers of pre-trained model avoid destroying any of the information they contain during future training rounds by setting trainable = FALSE.

To build our own training model, we first introduce the pre-trained model into our model, then reshaped the layer. Then we added a Long Short-Term Memory layer,

Then we added a dropout layer to prevent overfitting. The layer_global_max_pooling_1d() is used to downsample the input representation. Then we added a relu activation layer, and normalize the activations of the previous layer at each batch.

\
