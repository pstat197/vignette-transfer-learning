# Building the Model
model <- keras_model_sequential() %>%
USE.layer() %>%
layer_reshape(target_shape = c(-1, 512)) %>%
bidirectional(layer_lstm(units = 64, return_sequences = T, go_backwards = T)) %>%
layer_dropout(0.5) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(0.25) %>%
layer_dense(1, activation = "sigmoid")
library(keras)
library(tidyverse)
library(tidymodels)
library(tensorflow)
library(tfhub)
# Loading the pre-trained Universal Sentence Embedder
model.url <- "https://tfhub.dev/google/universal-sentence-encoder/4"
USE.layer <- tfhub::layer_hub(handle = model.url, trainable = FALSE,
name = "UniversalSentenceEmbeddingLayer")
library(keras)
library(tidyverse)
library(tidymodels)
library(tensorflow)
library(tfhub)
# Loading the pre-trained Universal Sentence Embedder
model.url <- "https://tfhub.dev/google/universal-sentence-encoder/4"
USE.layer <- tfhub::layer_hub(handle = model.url, trainable = FALSE,
name = "UniversalSentenceEmbeddingLayer")
# Load in data
load("Data/Data_Processed.RData")
# had to put labels in matrix
train.labels <- as.matrix(twitter.train.labels)
test.labels <- as.matrix(twitter.test.labels)
# Building the Model
model <- keras_model_sequential() %>%
USE.layer() %>%
layer_reshape(target_shape = c(-1, 512)) %>%
bidirectional(layer_lstm(units = 64, return_sequences = T, go_backwards = T)) %>%
layer_dropout(0.5) %>%
layer_global_max_pooling_1d() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_batch_normalization() %>%
layer_dropout(0.25) %>%
layer_dense(1, activation = "sigmoid")
# configure for training
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),
metrics = 'accuracy'
)
# train
history <- model %>%
fit(twitter.train.text,
train.labels,
validation_split = 0.3,
epochs = 5)
summary(model)
evaluate(model, twitter.test.text, test.labels)
load("~/Documents/2022Fall/Pstat197/vignette-transfer-learning/Data/Data_Processed.RData")
load("~/Documents/2022Fall/Pstat197/vignette-transfer-learning/Data/Reddit_Data_Processed.RData")
library(tidyverse)
library(tidymodels)
# Loading in data
reddit.data <- read.csv("Data/Reddit_Data.csv")
# encode data so that each column represents 1 if it that column's label
# 1 in the neutral column indicates neutral sentiment etc
reddit.data.encoded <- reddit.data %>%
mutate("Negative" = ifelse(category == -1, 1, 0),
"Neutral" = ifelse(category == 0, 1, 0),
"Positive" = ifelse(category == 1, 1, 0)) %>%
select(-category)
# train test split
set.seed(1111)
partition.reddit <- reddit.data.encoded %>%
initial_split(prop = 0.8)
reddit.train.text <- training(partition.reddit) %>%
pull(clean_comment)
reddit.train.labels.encoded <- training(partition.reddit) %>%
select(-clean_comment)
reddit.test.text <- testing(partition.reddit) %>%
pull(clean_comment)
reddit.test.labels.encoded <- testing(partition.reddit) %>%
select(-clean_comment)
partition.reddit <- reddit.data.encoded %>%
initial_split(prop = 0.8)
reddit.train.text <- training(partition.reddit) %>%
pull(clean_comment)
reddit.train.labels.encoded <- training(partition.reddit) %>%
select(-clean_comment)
reddit.test.text <- testing(partition.reddit) %>%
pull(clean_comment)
reddit.test.labels.encoded <- testing(partition.reddit) %>%
select(-clean_comment)
# loading in twitter data
twitter.data <- read.csv("Data/Twitter_Data.csv")
twitter.data <- twitter.data[complete.cases(twitter.data),]
# trying to remove all neutrals
twitter.data <- twitter.data[twitter.data$category != 0, ]
# set negative sentiment to 0 and positive to 1
twitter.data <- twitter.data %>%
mutate(sentiment = ifelse(category == -1, 0, 1)) %>%
select(-category)
set.seed(1111)
partition.twitter <- twitter.data %>%
initial_split(prop = 0.8)
twitter.train.text <- training(partition.twitter) %>%
pull(clean_text)
twitter.train.labels <- training(partition.twitter) %>%
select(-clean_text)
twitter.test.text <- testing(partition.twitter) %>%
pull(clean_text)
library(keras)
library(tidyverse)
library(tidymodels)
library(tensorflow)
library(tfhub)
